{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08498ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入模块\n",
    "import os\n",
    "# 设置代理。这里 1080 既可以是 http 代理的端口，也可以是 socks5 代理的端口\n",
    "proxy = 'http://127.0.0.1:7897'\n",
    "os.environ['http_proxy'] = proxy\n",
    "os.environ['HTTP_PROXY'] = proxy\n",
    "os.environ['https_proxy'] = proxy\n",
    "os.environ['HTTPS_PROXY'] = proxy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8803efa6-682e-4525-a5ab-511fda091cbb",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "# ViperGPT -- Quick Start\n",
    "\n",
    "This notebook is meant to be a quick entry into ViperGPT. **Warning:** This notebook will execute arbitrary code on your machine. Proceed at your own risk.\n",
    "\n",
    "Before running this code, modify any parameters at `configs/my_config.yaml`. For example, you may want to change the BLIP model (`blip_v2_model_type`) from XXL to XL if your GPU does not have enough memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c38a2623-4220-41ea-a1cb-a95ad2e41d83",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/csi/miniconda3/envs/vipergpt/lib/python3.10/site-packages/accelerate/utils/modeling.py:1462: UserWarning: Current model requires 4224 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fa3921686d14160ab20e73f031ca2d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "You are trying to offload the whole model to the disk. Please use the `disk_offload` function instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmain_simple_lib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/Repos/viper/main_simple_lib.py:32\u001b[0m\n\u001b[1;32m     29\u001b[0m cache \u001b[38;5;241m=\u001b[39m Memory(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcache/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     31\u001b[0m mp\u001b[38;5;241m.\u001b[39mset_start_method(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m, force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvision_processes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m forward, finish_all_consumers  \u001b[38;5;66;03m# This import loads all the models. May take a while\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimage_patch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvideo_segment\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/Repos/viper/vision_processes.py:179\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m process_name_ \u001b[38;5;129;01min\u001b[39;00m model_class_\u001b[38;5;241m.\u001b[39mlist_processes():\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m process_name_ \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mload_models \u001b[38;5;129;01mand\u001b[39;00m config\u001b[38;5;241m.\u001b[39mload_models[process_name_]:\n\u001b[0;32m--> 179\u001b[0m             consumers[process_name_] \u001b[38;5;241m=\u001b[39m \u001b[43mmake_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_class_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_name_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcounter_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m             counter_ \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    182\u001b[0m queues_in \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Repos/viper/vision_processes.py:45\u001b[0m, in \u001b[0;36mmake_fn\u001b[0;34m(model_class, process_name, counter)\u001b[0m\n\u001b[1;32m     42\u001b[0m num_gpus \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count()\n\u001b[1;32m     43\u001b[0m gpu_number \u001b[38;5;241m=\u001b[39m counter \u001b[38;5;241m%\u001b[39m num_gpus\n\u001b[0;32m---> 45\u001b[0m model_instance \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgpu_number\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgpu_number\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_function\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m process_name \u001b[38;5;241m!=\u001b[39m model_class\u001b[38;5;241m.\u001b[39mname:\n",
      "File \u001b[0;32m~/Repos/viper/vision_models.py:1140\u001b[0m, in \u001b[0;36mCodeLlama.__init__\u001b[0;34m(self, gpu_number)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gpu_number \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1139\u001b[0m         max_memory[gpu_number] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m-> 1140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# load_in_8bit=True,  # For some reason this results in OOM when doing forward pass\u001b[39;49;00m\n\u001b[1;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msequential\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/miniconda3/envs/vipergpt/lib/python3.10/site-packages/transformers/modeling_utils.py:4259\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4256\u001b[0m         device_map_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moffload_buffers\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   4258\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fsdp_enabled() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_deepspeed_zero3_enabled():\n\u001b[0;32m-> 4259\u001b[0m         \u001b[43mdispatch_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdevice_map_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4262\u001b[0m     hf_quantizer\u001b[38;5;241m.\u001b[39mpostprocess_model(model)\n",
      "File \u001b[0;32m~/miniconda3/envs/vipergpt/lib/python3.10/site-packages/accelerate/big_modeling.py:496\u001b[0m, in \u001b[0;36mdispatch_model\u001b[0;34m(model, device_map, main_device, state_dict, offload_dir, offload_index, offload_buffers, skip_keys, preload_module_classes, force_hooks)\u001b[0m\n\u001b[1;32m    494\u001b[0m         model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 496\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    497\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to offload the whole model to the disk. Please use the `disk_offload` function instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    498\u001b[0m         )\n\u001b[1;32m    499\u001b[0m \u001b[38;5;66;03m# Convert OrderedDict back to dict for easier usage\u001b[39;00m\n\u001b[1;32m    500\u001b[0m model\u001b[38;5;241m.\u001b[39mhf_device_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(device_map)\n",
      "\u001b[0;31mValueError\u001b[0m: You are trying to offload the whole model to the disk. Please use the `disk_offload` function instead."
     ]
    }
   ],
   "source": [
    "from main_simple_lib import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eff3e340",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_BASE'] = 'https://api.chatanywhere.tech/v1'\n",
    "os.environ['OPENAI_API_KEY'] = 'sk-86MpCURClgqJtTQkvFrOe9m6mLRmv93rdXd5vYs7cLDCzlj1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b1841c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "im = load_image('https://viper.cs.columbia.edu/static/images/kids_muffins.jpg')\n",
    "query = 'How many muffins can each kid have for it to be fair?'\n",
    "\n",
    "show_single_image(im)\n",
    "code = get_code(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2de1e4e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "execute_code(code, im, show_intermediate_steps=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vipergpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
