{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./javascript_results/testdev/results_0.csv')\n",
    "# 导入模块\n",
    "import os\n",
    "# 设置代理。这里 1080 既可以是 http 代理的端口，也可以是 socks5 代理的端口\n",
    "proxy = 'https://mobisense.tns:m0b1sensevpn@g2-jp4.go2https.com:20006'\n",
    "os.environ['http_proxy'] = proxy\n",
    "os.environ['HTTP_PROXY'] = proxy\n",
    "os.environ['https_proxy'] = proxy\n",
    "os.environ['HTTPS_PROXY'] = proxy\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,2'\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def run_program(parameters, input_type_, retrying=False):\n",
    "    from image_patch import ImagePatch, llm_query, best_image_match, distance, bool_to_yesno\n",
    "    from video_segment import VideoSegment\n",
    "\n",
    "    global queue_results\n",
    "\n",
    "    code, sample_id, image, possible_answers, query = parameters\n",
    "\n",
    "    code_header = f'def execute_command_{sample_id}(' \\\n",
    "                  f'{input_type_}, possible_answers, query, ' \\\n",
    "                  f'ImagePatch, VideoSegment, ' \\\n",
    "                  'llm_query, bool_to_yesno, distance, best_image_match):\\n' \\\n",
    "                  f'    # Answer is:'\n",
    "    code = code_header + code\n",
    "\n",
    "    try:\n",
    "        exec(compile(code, 'Codex', 'exec'), globals())\n",
    "    except Exception as e:\n",
    "        print(f'Sample {sample_id} failed at compilation time with error: {e}')\n",
    "        \n",
    "\n",
    "   \n",
    "\n",
    "    image_patch_partial = partial(ImagePatch)\n",
    "    video_segment_partial = partial(VideoSegment)\n",
    "    llm_query_partial = partial(llm_query)\n",
    "\n",
    "    try:\n",
    "        result = globals()[f'execute_command_{sample_id}'](\n",
    "            # Inputs to the function\n",
    "            image, possible_answers, query,\n",
    "            # Classes to be used\n",
    "            image_patch_partial, video_segment_partial,\n",
    "            # Functions to be used\n",
    "            llm_query_partial, bool_to_yesno, distance, best_image_match)\n",
    "    except Exception as e:\n",
    "        # print full traceback\n",
    "        \n",
    "        if retrying:\n",
    "            return None, code\n",
    "        print(f'Sample {sample_id} failed with error: {e}. Next you will see an \"expected an indented block\" error. ')\n",
    "        # Retry again with fixed code\n",
    "        new_code = \"[\"  # This code will break upon execution, and it will be caught by the except clause\n",
    "        result = run_program((new_code, sample_id, image, possible_answers, query), input_type_,\n",
    "                             retrying=True)[0]\n",
    "\n",
    "    # The function run_{sample_id} is defined globally (exec doesn't work locally). A cleaner alternative would be to\n",
    "    # save it in a global dict (replace globals() for dict_name in exec), but then it doesn't detect the imported\n",
    "    # libraries for some reason. Because defining it globally is not ideal, we just delete it after running it.\n",
    "    if f'execute_command_{sample_id}' in globals():\n",
    "        del globals()[f'execute_command_{sample_id}']  # If it failed to compile the code, it won't be defined\n",
    "    return result, code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsconvert.transpiler as trans\n",
    "\n",
    "\n",
    "idx = 2\n",
    "code = df.loc[idx]['code'].split('\\'')[1]\n",
    "python_code = trans.format_code(code)\n",
    "print(code)\n",
    "print(python_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    e = ImagePatch(image)\n",
      "    return e.llm_query(\"Is it overcast?\", not 1)\n",
      "\n",
      "\n",
      "<class 'vision_models.BLIPModel'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cf3f71f7c6e4631a4bdd7f867e1c3a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'vision_models.CodexModel'>\n",
      "<class 'vision_models.DepthEstimationModel'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/intel-isl_MiDaS_master\n",
      "Using cache found in /root/.cache/torch/hub/intel-isl_MiDaS_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'vision_models.GLIPModel'>\n",
      "VISION BACKBONE USE GRADIENT CHECKPOINTING:  False\n",
      "LANGUAGE BACKBONE USE GRADIENT CHECKPOINTING:  False\n",
      "EARLY FUSION ON, USING MHA-B\n",
      "EARLY FUSION ON, USING MHA-B\n",
      "EARLY FUSION ON, USING MHA-B\n",
      "EARLY FUSION ON, USING MHA-B\n",
      "EARLY FUSION ON, USING MHA-B\n",
      "EARLY FUSION ON, USING MHA-B\n",
      "EARLY FUSION ON, USING MHA-B\n",
      "EARLY FUSION ON, USING MHA-B\n",
      "<class 'vision_models.GPT3Model'>\n",
      "<class 'vision_models.GPT3Model'>\n",
      "<class 'vision_models.XVLMModel'>\n",
      "execute here\n",
      "gpt-3.5-turbo\n",
      "ChatCompletion(id='chatcmpl-AQ3xM4Fwi5GhC2FUwAwui6kwb0TqE', choices=[Choice(finish_reason='stop', index=0, logprobs=ChoiceLogprobs(content=[ChatCompletionTokenLogprob(token='yes', bytes=[121, 101, 115], logprob=-0.26087588, top_logprobs=[])], refusal=None), message=ChatCompletionMessage(content='yes', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1730775100, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=142, total_tokens=143, completion_tokens_details=CompletionTokensDetails(audio_tokens=None, reasoning_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0)))\n",
      "Error in gpt3_qa model: 'ChatCompletion' object is not subscriptable\n",
      "\n",
      "    e = ImagePatch(image)\n",
      "    t = e.find(\"dress\")\n",
      "    return t[0].llm_query(\"Who is wearing the dress?\", not 1)\n",
      "\n",
      "\n",
      "execute here\n",
      "gpt-3.5-turbo\n",
      "ChatCompletion(id='chatcmpl-AQ3xNSxvKZu9PVLSXUKoaYh2zRfHG', choices=[Choice(finish_reason='stop', index=0, logprobs=ChoiceLogprobs(content=[ChatCompletionTokenLogprob(token='the', bytes=[116, 104, 101], logprob=-0.9482134, top_logprobs=[]), ChatCompletionTokenLogprob(token=' woman', bytes=[32, 119, 111, 109, 97, 110], logprob=-0.48539653, top_logprobs=[])], refusal=None), message=ChatCompletionMessage(content='the woman', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1730775101, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=2, prompt_tokens=143, total_tokens=145, completion_tokens_details=CompletionTokensDetails(audio_tokens=None, reasoning_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0)))\n",
      "Error in gpt3_qa model: 'ChatCompletion' object is not subscriptable\n",
      "\n",
      "    imagePatch = ImagePatch(image)\n",
      "    utensilPatches = imagePatch.find(\"utensil\")\n",
      "    utensilPatch = utensilPatches[0]\n",
      "    tablePatches = imagePatch.find(\"table\")\n",
      "    tablePatch = tablePatches[0]\n",
      "    return bool_to_yesno(utensilPatch.verify_property(\"utensil\", \"clean\") and utensilPatch.verify_property(\"utensil\", \"black\") and utensilPatch.overlaps_with(tablePatch.left, tablePatch.lower, tablePatch.right, tablePatch.upper))\n",
      "\n",
      "\n",
      "\n",
      "    e = ImagePatch(image)\n",
      "    t = e.find(\"surfer\")\n",
      "    n = t[0]\n",
      "    r = n.llm_query(\"Is the surfer wearing a wetsuit?\", not 1)\n",
      "    return bool_to_yesno(r)\n",
      "\n",
      "\n",
      "execute here\n",
      "gpt-3.5-turbo\n",
      "ChatCompletion(id='chatcmpl-AQ3xPz4NCTYo7G517Wd7U3N9px2ss', choices=[Choice(finish_reason='stop', index=0, logprobs=ChoiceLogprobs(content=[ChatCompletionTokenLogprob(token='yes', bytes=[121, 101, 115], logprob=-0.47427422, top_logprobs=[])], refusal=None), message=ChatCompletionMessage(content='yes', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1730775103, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=147, total_tokens=148, completion_tokens_details=CompletionTokensDetails(audio_tokens=None, reasoning_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0)))\n",
      "Error in gpt3_qa model: 'ChatCompletion' object is not subscriptable\n",
      "\n",
      "    e = ImagePatch(image)\n",
      "    t = e.find(\"chair\")\n",
      "    return t[0].compute_depth()\n",
      "\n",
      "\n",
      "\n",
      "    imagePatch = ImagePatch(image)\n",
      "    deskPatches = imagePatch.find(\"desk\")\n",
      "    deskPatch = deskPatches[0]\n",
      "    devicePatches = deskPatch.find(\"device\")\n",
      "    devicePatch = devicePatches[0]\n",
      "    return devicePatch.simpleQuery(\"What kind of device is this?\")\n",
      "\n",
      "\n",
      "Sample 201902997 failed with error: 'ImagePatch' object has no attribute 'simpleQuery'. Next you will see an \"expected an indented block\" error. \n",
      "Sample 201902997 failed at compilation time with error: expected an indented block after function definition on line 1 (Codex, line 2)\n",
      "\n",
      "    e = ImagePatch(image)\n",
      "    t = e.find(\"airplane\")\n",
      "    return t[0].simple_query(\"What is this?\")\n",
      "\n",
      "\n",
      "\n",
      "    e = ImagePatch(image)\n",
      "    n = e.find(\"pants\")\n",
      "    return n[0].simple_query(\"What is the color?\")\n",
      "\n",
      "\n",
      "\n",
      "    e = ImagePatch(image)\n",
      "    return e.llm_query(\"What is the color of the ground?\", not 1)\n",
      "\n",
      "\n",
      "execute here\n",
      "gpt-3.5-turbo\n",
      "ChatCompletion(id='chatcmpl-AQ3xUfP6xjJ31o3ZbNx43u6PcBto1', choices=[Choice(finish_reason='stop', index=0, logprobs=ChoiceLogprobs(content=[ChatCompletionTokenLogprob(token='brown', bytes=[98, 114, 111, 119, 110], logprob=-0.039543726, top_logprobs=[])], refusal=None), message=ChatCompletionMessage(content='brown', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1730775108, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=145, total_tokens=146, completion_tokens_details=CompletionTokensDetails(audio_tokens=None, reasoning_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0)))\n",
      "Error in gpt3_qa model: 'ChatCompletion' object is not subscriptable\n",
      "\n",
      "    e = ImagePatch(image)\n",
      "    return e.llm_query(\"What is around the open window?\", not 1)\n",
      "\n",
      "\n",
      "execute here\n",
      "gpt-3.5-turbo\n",
      "ChatCompletion(id='chatcmpl-AQ3xUl2afzCByYQIP7snVBAQfItjw', choices=[Choice(finish_reason='stop', index=0, logprobs=ChoiceLogprobs(content=[ChatCompletionTokenLogprob(token='air', bytes=[97, 105, 114], logprob=-0.32310417, top_logprobs=[])], refusal=None), message=ChatCompletionMessage(content='air', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1730775108, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=144, total_tokens=145, completion_tokens_details=CompletionTokensDetails(audio_tokens=None, reasoning_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0)))\n",
      "Error in gpt3_qa model: 'ChatCompletion' object is not subscriptable\n"
     ]
    }
   ],
   "source": [
    "import jsconvert.transpiler as trans\n",
    "from PIL import Image\n",
    "\n",
    "for idx in range(0, 10):\n",
    "    code = df.loc[idx]['code'].split('\\'')[1]\n",
    "    python_code = trans.format_code(code)\n",
    "    code = python_code.split('def execute_command(image):')[1] # df.loc[idx]['code'].split('\\'')[1].split('def execute_command(image):')[1].replace('\\\\n', '\\n')\n",
    "    print(code)\n",
    "    sample_id = df.loc[idx]['id']\n",
    "    image = df.loc[idx]['img_path']\n",
    "    with open(image, \"rb\") as f:\n",
    "        pil_img = Image.open(f).convert(\"RGB\")\n",
    "    possible_answers = df.loc[idx]['possible_answers']\n",
    "    query = df.loc[idx]['query']\n",
    "    parameters = code, sample_id, pil_img, possible_answers, query\n",
    "    result, code = run_program(parameters, 'image')\n",
    "    df.loc[idx, 'result'] = result\n",
    "df['result'] = df['result'].apply(str)\n",
    "df.to_csv('evaluate_javascript.csv', header=True, index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vipergpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
